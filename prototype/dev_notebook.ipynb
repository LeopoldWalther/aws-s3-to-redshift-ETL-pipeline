{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Warehouse on AWS using the AWS python SDK \n",
    "## Notebook for development purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import configparser\n",
    "from botocore.exceptions import ClientError\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# STEP 0:  AWS secret and access key\n",
    "\n",
    "- Create a new IAM user in your AWS account\n",
    "- Give it `AdministratorAccess`, From `Attach existing policies directly` Tab\n",
    "- Take note of the access key and secret \n",
    "- Edit the file `dwh.cfg` in the same folder as this notebook and fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DWH Params from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY');\n",
    "SECRET                 = config.get('AWS','SECRET');\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\");\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\");\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\");\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\");\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\");\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\");\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\");\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\");\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create clients for EC2, S3, IAM, and Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2 = boto3.resource('ec2',\n",
    "                    region_name='us-west-2',\n",
    "                    aws_access_key_id=KEY,\n",
    "                    aws_secret_access_key=SECRET\n",
    "                   )\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                  region_name='us-west-2',\n",
    "                  aws_access_key_id=KEY,\n",
    "                  aws_secret_access_key=SECRET\n",
    "                  ) \n",
    "\n",
    "iam = boto3.client('iam',\n",
    "                   region_name='us-west-2',\n",
    "                   aws_access_key_id=KEY,\n",
    "                   aws_secret_access_key=SECRET\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name='us-west-2',\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## STEP 1: IAM ROLE\n",
    "- Create an IAM Role that makes Redshift able to access S3 bucket (ReadOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1 Create the role, \n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "       \n",
    "print(\"1.2 Attaching Policy\")\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2:  Redshift Cluster\n",
    "\n",
    "- Create a RedShift Cluster\n",
    "- For complete arguments to `create_cluster`, see [docs](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.create_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## 2.1 Describe the cluster to see its status\n",
    "- run this block several times until the cluster status becomes `Available`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "    \n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "<h2> 2.2 Get the cluster endpoint and role ARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>DO NOT RUN THIS unless the cluster status becomes \"Available\" </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print('DWH_ENDPOINT and DWH_ROLE_ARN successfully retrieved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set('DWH', 'DWH_ENDPOINT', DWH_ENDPOINT)\n",
    "config.set('DWH', 'DWH_ROLE_ARN', DWH_ROLE_ARN)\n",
    "with open('dwh_copy.cfg', 'w') as configfile:    #    \n",
    "    config.write(configfile)\n",
    "    config.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Open an incoming  TCP port to access the cluster ednpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: Connect to the clusterConnect to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "# print(conn_string)\n",
    "%sql $conn_string\n",
    "print(\"Connected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: Create Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1 Create tables to load S3 data into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS leo;\n",
    "SET search_path TO leo;\n",
    "\n",
    "DROP TABLE IF EXISTS staging_events;\n",
    "DROP TABLE IF EXISTS staging_songs;\n",
    "\n",
    "CREATE TABLE staging_events(\n",
    "    artist              VARCHAR,\n",
    "    auth                VARCHAR,\n",
    "    firstName           VARCHAR,\n",
    "    gender              VARCHAR,\n",
    "    itemInSession       INTEGER,\n",
    "    lastName            VARCHAR,\n",
    "    length              FLOAT,\n",
    "    level               VARCHAR,\n",
    "    location            VARCHAR,\n",
    "    method              VARCHAR,\n",
    "    page                VARCHAR,\n",
    "    registration        FLOAT,\n",
    "    sessionId           INTEGER,\n",
    "    song                VARCHAR,\n",
    "    status              INTEGER,\n",
    "    ts                  TIMESTAMP,\n",
    "    userAgent           VARCHAR,\n",
    "    userId              INTEGER \n",
    ");\n",
    "\n",
    "CREATE TABLE staging_songs(\n",
    "    num_songs           INTEGER,\n",
    "    artist_id           VARCHAR,\n",
    "    artist_latitude     FLOAT,\n",
    "    artist_longitude    FLOAT,\n",
    "    artist_location     VARCHAR,\n",
    "    artist_name         VARCHAR,\n",
    "    song_id             VARCHAR,\n",
    "    title               VARCHAR,\n",
    "    duration            FLOAT,\n",
    "    year                INTEGER\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Fill tables with data from JSON files in S3 buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_events_copy = (\"\"\"\n",
    "    copy staging_events from {data_bucket}\n",
    "    credentials 'aws_iam_role={role_arn}'\n",
    "    region 'us-west-2' format as JSON {log_json_path}\n",
    "    timeformat as 'epochmillisecs';\n",
    "\"\"\").format(data_bucket=config['S3']['LOG_DATA'], role_arn=DWH_ROLE_ARN, log_json_path=config['S3']['LOG_JSONPATH'])\n",
    "\n",
    "staging_songs_copy = (\"\"\"\n",
    "    copy staging_songs from {data_bucket}\n",
    "    credentials 'aws_iam_role={role_arn}'\n",
    "    region 'us-west-2' format as JSON 'auto';\n",
    "\"\"\").format(data_bucket=config['S3']['SONG_DATA'], role_arn=DWH_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql $staging_events_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql $staging_songs_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check how many rows these tables have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT COUNT(*) FROM staging_events;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT COUNT(*) FROM staging_songs;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 4.3: Create star schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "DROP TABLE IF EXISTS songplays;\n",
    "DROP TABLE IF EXISTS users;\n",
    "DROP TABLE IF EXISTS songs;\n",
    "DROP TABLE IF EXISTS artists;\n",
    "DROP TABLE IF EXISTS time;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS songplays(\n",
    "    songplay_id         INTEGER     IDENTITY(0,1)   PRIMARY KEY, \n",
    "    start_time          TIMESTAMP   NOT NULL        SORTKEY DISTKEY,\n",
    "    user_id             INTEGER     NOT NULL, \n",
    "    level               VARCHAR,\n",
    "    song_id             VARCHAR     NOT NULL, \n",
    "    artist_id           VARCHAR     NOT NULL,  \n",
    "    session_id          INTEGER, \n",
    "    location            VARCHAR, \n",
    "    user_agent          VARCHAR\n",
    "    );\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS users(\n",
    "    user_id             INTEGER     NOT NULL    SORTKEY PRIMARY KEY, \n",
    "    first_name          VARCHAR     NOT NULL, \n",
    "    last_name           VARCHAR     NOT NULL,\n",
    "    gender              VARCHAR     NOT NULL, \n",
    "    level               VARCHAR     NOT NULL\n",
    "    )diststyle all;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS songs(\n",
    "    song_id             VARCHAR     NOT NULL    SORTKEY PRIMARY KEY,\n",
    "    title               VARCHAR     NOT NULL, \n",
    "    artist_id           VARCHAR     NOT NULL,  \n",
    "    year                INTEGER     NOT NULL,  \n",
    "    duration            FLOAT\n",
    "    );\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS artists(\n",
    "    artist_id           VARCHAR     NOT NULL    SORTKEY PRIMARY KEY,\n",
    "    name                VARCHAR     NOT NULL,\n",
    "    location            VARCHAR,\n",
    "    latitude            FLOAT,\n",
    "    longitude           FLOAT\n",
    "    );\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS time(\n",
    "    start_time          TIMESTAMP   NOT NULL    DISTKEY SORTKEY PRIMARY KEY,\n",
    "    hour                INTEGER     NOT NULL, \n",
    "    day                 INTEGER     NOT NULL, \n",
    "    week                INTEGER     NOT NULL, \n",
    "    month               INTEGER     NOT NULL, \n",
    "    year                INTEGER     NOT NULL, \n",
    "    weekday             VARCHAR     NOT NULL\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 4.4: Fill the facts and dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "INSERT INTO songplays (start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)\n",
    "SELECT  DISTINCT(e.ts)  AS start_time, \n",
    "        e.userId        AS user_id, \n",
    "        e.level         AS level, \n",
    "        s.song_id       AS song_id, \n",
    "        s.artist_id     AS artist_id, \n",
    "        e.sessionId     AS session_id, \n",
    "        e.location      AS location, \n",
    "        e.userAgent     AS user_agent\n",
    "FROM staging_events e\n",
    "JOIN staging_songs  s   \n",
    "ON (e.song = s.title AND e.artist = s.artist_name)\n",
    "WHERE s.song_id IS NOT NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "INSERT INTO users (user_id, first_name, last_name, gender, level)\n",
    "SELECT  DISTINCT(userId)    AS user_id,\n",
    "        firstName           AS first_name,\n",
    "        lastName            AS last_name,\n",
    "        gender,\n",
    "        level\n",
    "FROM staging_events\n",
    "WHERE user_id IS NOT NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "INSERT INTO songs (song_id, title, artist_id, year, duration)\n",
    "SELECT  DISTINCT(song_id) AS song_id,\n",
    "        title,\n",
    "        artist_id,\n",
    "        year,\n",
    "        duration\n",
    "FROM staging_songs\n",
    "WHERE song_id IS NOT NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "INSERT INTO artists (artist_id, name, location, latitude, longitude)\n",
    "SELECT  DISTINCT(artist_id) AS artist_id,\n",
    "        artist_name         AS name,\n",
    "        artist_location     AS location,\n",
    "        artist_latitude     AS latitude,\n",
    "        artist_longitude    AS longitude\n",
    "FROM staging_songs\n",
    "WHERE artist_id IS NOT NULL;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "INSERT INTO time (start_time, hour, day, week, month, year, weekday)\n",
    "SELECT  DISTINCT(start_time)                AS start_time,\n",
    "        EXTRACT(hour FROM start_time)       AS hour,\n",
    "        EXTRACT(day FROM start_time)        AS day,\n",
    "        EXTRACT(week FROM start_time)       AS week,\n",
    "        EXTRACT(month FROM start_time)      AS month,\n",
    "        EXTRACT(year FROM start_time)       AS year,\n",
    "        EXTRACT(dayofweek FROM start_time)  as weekday\n",
    "FROM songplays;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: Clean up your resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color='red'>DO NOT RUN THIS UNLESS YOU ARE SURE <br/> \n",
    "    We will be using these resources in the next exercises</span></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run this block several times until the cluster really deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
